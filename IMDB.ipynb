{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZozl/WYHCZQyNEjCCi6Lt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajnishkumar1906/Deep-Learning/blob/main/IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall preinstalled versions\n",
        "!pip uninstall -y torch torchvision torchaudio torchtext\n",
        "\n",
        "# Install compatible versions (example for torch 2.1.2)\n",
        "# Allow pip to choose a compatible torchtext version\n",
        "!pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 torchtext"
      ],
      "metadata": {
        "id": "fAC9WHing9yK",
        "outputId": "5fb6cf9b-68cf-4207-d759-0fc106e01e67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.1.2\n",
            "Uninstalling torch-2.1.2:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#This project performs sentiment analysis on IMDB movie reviews using a Feedforward Neural Network (FFNN). The goal is to classify each review as positive or negative based on its text content.\n",
        "\n",
        ">Tokenize and vectorize the text using a fixed-size vocabulary.\n",
        "\n",
        ">Convert reviews into binary vectors (indicating presence of words).\n",
        "\n",
        ">Train a simple FFNN with one hidden layer.\n",
        "\n",
        ">Evaluate model performance on a test set.\n",
        "\n",
        ">Use the trained model to predict sentiment on new reviews.\n",
        "\n",
        ">This project introduces basic deep learning concepts for NLP, using FFNN     instead of more complex models like RNNs or LSTMs."
      ],
      "metadata": {
        "id": "fvi7U1He5Z-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "xpNPG8xWvRQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2CTKex3YhBMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load IMDB dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DATASETS/IMDB Dataset.csv\")\n",
        "\n",
        "# Convert sentiment labels to binary (positive:1, negative:0)\n",
        "df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "df = df[['review', 'label']]"
      ],
      "metadata": {
        "id": "6p7OK090qK-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rvw = df['review'][0]\n",
        "len(rvw)"
      ],
      "metadata": {
        "id": "hi4p0qyTxzxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize and vocabulary"
      ],
      "metadata": {
        "id": "VpiG94sVsDNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Build vocabulary from most common tokens\n",
        "VOCAB_SIZE = 5000\n",
        "specials = ['<PAD>', '<UNK>']\n",
        "counter = Counter()\n",
        "\n",
        "for review in df['review']:\n",
        "    counter.update(tokenizer(review))\n",
        "\n",
        "# Assign indices to words\n",
        "vocab = {word: idx + len(specials) for idx, (word, _) in enumerate(counter.most_common(VOCAB_SIZE - len(specials)))}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n"
      ],
      "metadata": {
        "id": "KtGRRQ3wqmWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode Reviews as Sequences of Token IDs\n",
        "\n"
      ],
      "metadata": {
        "id": "jUfyP0uetaXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "MAX_LEN = 200\n",
        "\n",
        "def encode_review(text):\n",
        "    tokens = tokenizer(text)\n",
        "    return [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
        "\n",
        "encoded = [torch.tensor(encode_review(review), dtype=torch.long) for review in df['review']]\n",
        "\n",
        "padded = pad_sequence(\n",
        "    [r[:MAX_LEN] if len(r) > MAX_LEN else torch.cat([r, torch.zeros(MAX_LEN - len(r), dtype=torch.long)])\n",
        "     for r in encoded],\n",
        "    batch_first=True\n",
        ")\n",
        "\n",
        "X = padded\n",
        "y = torch.tensor(df['label'].values, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "i1RmK4eAsKAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-Test Split"
      ],
      "metadata": {
        "id": "UQuUfOUPtg3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx = int(0.8 * len(X))\n",
        "X_train, X_test = X[:split_idx], X[split_idx:]\n",
        "y_train, y_test = y[:split_idx], y[split_idx:]"
      ],
      "metadata": {
        "id": "OUqksT2DteV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "nTFW2RSRt4Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=64)"
      ],
      "metadata": {
        "id": "fTxsKYketj4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "MzhcSXvdt-sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SimpleTextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)        # (batch, seq_len, embed_dim)\n",
        "        x = x.mean(dim=1)            # average pooling over sequence\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "bQBvgO0guA4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize model"
      ],
      "metadata": {
        "id": "Si3hP1VXuHte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SimpleTextClassifier(len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "Of7Cf31juCSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "O5QSb89TuXLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 5\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} - Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Rx0XcBujuKd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate accuracy"
      ],
      "metadata": {
        "id": "NI4kyhtnu1lV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Cq3WFWgmuaYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom predictions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "380_-n-Gu9H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "    model.eval()\n",
        "    tokens = encode_review(text)\n",
        "    if len(tokens) < MAX_LEN:\n",
        "        tokens += [0] * (MAX_LEN - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:MAX_LEN]\n",
        "    input_tensor = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        pred = torch.argmax(output, dim=1).item()\n",
        "    return \"positive\" if pred == 1 else \"negative\"\n",
        "\n",
        "print(predict(\"This movie was absolutely amazing!\"))\n",
        "print(predict(\"Worst plot ever. Waste of time.\"))\n"
      ],
      "metadata": {
        "id": "YzHL1KHNu4ct"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}