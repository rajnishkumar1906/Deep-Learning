{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6c4ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3845c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"D:\\hindi_english_parallel.csv\").dropna().head(100000)\n",
    "\n",
    "# Clean Hindi\n",
    "def clean_hindi(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r\"[^\\u0900-\\u097F\\s]\", \"\", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# Clean English\n",
    "def clean_english(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text.lower())\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "data[\"hindi\"] = data[\"hindi\"].apply(clean_hindi)\n",
    "data[\"english\"] = data[\"english\"].apply(clean_english)\n",
    "data[\"english\"] = data[\"english\"].apply(lambda x: \"<sos> \" + x + \" <eos>\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3409f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e77d052b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(texts):\n",
    "    for sentence in texts:\n",
    "        yield sentence.split()\n",
    "\n",
    "# Hindi Vocab\n",
    "hindi_vocab = build_vocab_from_iterator(yield_tokens(train_data[\"hindi\"]), specials=[\"<pad>\", \"<unk>\"])\n",
    "hindi_vocab.set_default_index(hindi_vocab[\"<unk>\"])\n",
    "\n",
    "# English Vocab\n",
    "english_vocab = build_vocab_from_iterator(yield_tokens(train_data[\"english\"]), specials=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"])\n",
    "english_vocab.set_default_index(english_vocab[\"<unk>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f64a001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, src_vocab, tgt_vocab):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.data.iloc[idx][\"hindi\"]\n",
    "        tgt = self.data.iloc[idx][\"english\"]\n",
    "        \n",
    "        src_tensor = torch.tensor([self.src_vocab[token] for token in src.split()], dtype=torch.long)\n",
    "        tgt_tensor = torch.tensor([self.tgt_vocab[token] for token in tgt.split()], dtype=torch.long)\n",
    "\n",
    "        return src_tensor, tgt_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1ffb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value=hindi_vocab[\"<pad>\"], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=english_vocab[\"<pad>\"], batch_first=True)\n",
    "    \n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1103b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(train_data, hindi_vocab, english_vocab)\n",
    "val_dataset = TranslationDataset(val_data, hindi_vocab, english_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b4c8fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)  # [batch, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)  # LSTM returns (hidden, cell)\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77848016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)  # [batch, 1]\n",
    "        embedded = self.embedding(input)  # [batch, 1, emb_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))  # [batch, output_dim]\n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcbbc46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = tgt[:, 0]  # <sos>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            input = tgt[:, t] if teacher_force else output.argmax(1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9abc96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set up GPU usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3928f0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rajnish Kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "INPUT_DIM = len(hindi_vocab)\n",
    "OUTPUT_DIM = len(english_vocab)\n",
    "EMB_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 1\n",
    "DROPOUT = 0.3\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5acdf4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "PAD_IDX = english_vocab[\"<pad>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71338225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    loop = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch_idx, (src, tgt) in enumerate(loop):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)  # output: [batch, tgt_len, output_dim]\n",
    "\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar description\n",
    "        loop.set_postfix(batch=batch_idx + 1, loss=loss.item())\n",
    "\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd57b7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 3.8295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss: 1.5059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss: 0.8499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss: 0.6275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss: 0.5204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss: 0.4582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss: 0.4224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss: 0.4027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Train Loss: 0.3839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.3648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe3c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, hindi_vocab, english_vocab, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert Hindi sentence to indices\n",
    "    tokens = sentence.lower().split()\n",
    "    input_ids = [hindi_vocab[token] for token in tokens]\n",
    "    input_ids = input_ids[:max_len]\n",
    "    input_ids += [hindi_vocab[\"<pad>\"]] * (max_len - len(input_ids))\n",
    "    src_tensor = torch.LongTensor([input_ids]).to(device)  # shape: [1, max_len]\n",
    "\n",
    "    # Start decoding with <sos>\n",
    "    tgt_input = torch.LongTensor([english_vocab[\"<sos>\"]]).unsqueeze(0).to(device)  # shape: [1, 1]\n",
    "\n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = model.decoder(tgt_input, hidden, cell)  # output shape: [1, 1, vocab_size]\n",
    "            pred_token = output.argmax(2)[:, -1].item()\n",
    "\n",
    "            if pred_token == english_vocab[\"<eos>\"]:\n",
    "                break\n",
    "            outputs.append(pred_token)\n",
    "\n",
    "            # Update input\n",
    "            tgt_input = torch.LongTensor([pred_token]).unsqueeze(0).to(device)\n",
    "\n",
    "    # Convert predicted indices to words\n",
    "    inv_vocab = {idx: word for word, idx in english_vocab.get_stoi().items()}\n",
    "    predicted_words = [inv_vocab.get(idx, \"<unk>\") for idx in outputs]\n",
    "    return ' '.join(predicted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc83707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_bleu(model, val_df, hindi_vocab, english_vocab, num_samples=100):\n",
    "    scores = []\n",
    "    smooth = SmoothingFunction().method4\n",
    "\n",
    "    for i in range(min(num_samples, len(val_df))):\n",
    "        src = val_df.iloc[i][\"hindi\"]\n",
    "        tgt = val_df.iloc[i][\"english\"]\n",
    "\n",
    "        ref = tgt.lower().split()\n",
    "        pred = translate_sentence(src, model, hindi_vocab, english_vocab).split()\n",
    "\n",
    "        bleu = sentence_bleu([ref], pred, smoothing_function=smooth)\n",
    "        scores.append(bleu)\n",
    "\n",
    "        # Show every 10th example\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[{i}]\")\n",
    "            print(\"Hindi:    \", src)\n",
    "            print(\"Target:   \", tgt)\n",
    "            print(\"Predicted:\", ' '.join(pred))\n",
    "            print(\"BLEU:     \", round(bleu, 4), \"\\n\")\n",
    "\n",
    "    avg_bleu = np.mean(scores)\n",
    "    print(f\"\\nâœ… Average BLEU score on {num_samples} samples: {round(avg_bleu, 4)}\")\n",
    "    return avg_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d349ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bleu = evaluate_bleu(model, val_data, hindi_vocab, english_vocab, num_samples=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979753b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
